# data-unit-test
data-unit-test is a framework that allows user to test their data present in any database. This database can be hive, Redshift, Athena or even MySQL. The users provide a YAML file that will contain all the tests specifications. The framework parses the YAML input file and creates the executable tests. These tests are executed against the database provided by the user and the results are validated against the expected results provided by the user. The framework uses TestNG and each test is run as a JUNIT test that is run via the TestNG framework. The test report is also generated for the users to view the output of their test cases.

## Build and Run
The project requires sbt to be installed on the local system. Once sbt is installed, user can build it locally to generate the uber jar file. This jar can then be used to run using the run.sh script provided with the project. Once the project is forked, user can use following command to clone and build.

- `$ git clone <github_repo>`
- `$ cd <github_repo>`
- `$ sbt clean assembly`
The last command will generate the uber jar in target folder. The user can then create the YAML file containing all the tests. Sample YAML files can be found in examples directory of the project. 
For running the test cases see [Executing Tests](#executing-tests)

## Generating JavaDocs
The java doc can be generated by sbt. Following command can be used for that.
- `$ sbt clean doc`

### Notes
The java docs generated will be in directory `/target/api` 

## Supported tests
Currently following test scenarios are supported
<table>
  <tbody>
    <tr>
      <th>Name</th>
      <th>Description</th>
      <th>Expectation</th>
      <th colspan="1">Example</th>
    </tr>
    <tr>
      <td>assert_equals</td>
      <td>Tests for the equality of actual and expected data. Note that this does not checks for the ordering of data. In a hive table, there can be millions of rows which are processed in a distributed fashion. Hence the ordering of results cannot be guaranteed. Hence, the equality check disregards the order of results and compares the expected and actual results.</td>
      <td>
        <p>LIST. Expects a list of list. Which means the user can provide the list of rows with comma separated column values.</p>
        <p>or</p>
        <p>MAP. Expects a list of map. Which means that one has to provide each row in form of map where the keys are the column names and values are the column values. Since there can be multiple rows, hence user will provide a list of such maps where each map correspond to a row</p>
        <p>
          <span> or</span>
        </p>
        <p>CSV <span>file name</span>. Expects a csv file with header line containing the column names and the body contains the row values. Each line of the file corresponds to a row.</p>
      </td>
      <td colspan="1">
        <p>LIST:</p>
        <p>- [dt=20170202/dh=00]<br/>- [dt=20170202/dh=01]<br/>- [dt=20170202/dh=02]</p>
        <p> </p>
        <p>MAP</p>
        <p>- <br/> id: 1<br/> name: name1<br/> -<br/> id: 2<br/> name: name2<br/> -<br/> id: 3<br/> name: name3<br/> -<br/> id: 4<br/> name: name4<br/> -<br/> id: 5<br/> name: name5</p>
        <p> </p>
        <p>CSV</p>
        <p>src/test/resources/expected_equals.csv</p>
      </td>
    </tr>
    <tr>
      <td> assert_includes</td>
      <td> Tests for the inclusion of the expected results in the actual resultset obtained by querying the data. This does not checks for the ordering of data for the same reason as above.</td>
      <td> LIST. Expects a list of list. Which means the user can provide the list of rows with comma separated column values.<p>or</p>
        <p>MAP. Expects a list of map. Which means that one has to provide each row in form of map where the keys are the column names and values are the column values. Since there can be multiple rows, hence user will provide a list of such maps where each map correspond to a row</p>
        <p>or</p>
        <p>CSV <span>file name</span>. Expects a csv file with header line containing the column names and the body contains the row values. Each line of the file corresponds to a row.</p>
      </td>
      <td colspan="1"> <p>LIST:</p>
        <p>- [dt=20170202/dh=00]<br/>- [dt=20170202/dh=01]<br/>- [dt=20170202/dh=02]</p>
        <p> </p>
        <p>MAP</p>
        <p>- <br/> id: 1<br/> name: name1<br/> -<br/> id: 2<br/> name: name2<br/> -<br/> id: 3<br/> name: name3<br/> -<br/> id: 4<br/> name: name4<br/> -<br/> id: 5<br/> name: name5</p>
        <p> </p>
        <p>CSV</p>
        <p>src/test/resources/expected_equals.csv</p>
      </td>
    </tr>
    <tr>
      <td colspan="1"> assert_excludes</td>
      <td colspan="1">Tests for the exclusion of the expected results in the actual resultset obtained by querying the data. This does not checks for the ordering of data for the same reason as above.</td>
      <td colspan="1"> LIST. Expects a list of list. Which means the user can provide the list of rows with comma separated column values.<p>or</p>
        <p>MAP. Expects a list of map. Which means that one has to provide each row in form of map where the keys are the column names and values are the column values. Since there can be multiple rows, hence user will provide a list of such maps where each map correspond to a row</p>
        <p>or</p>
        <p>CSV file name. Expects a csv file with header line containing the column names and the body contains the row values. Each line of the file corresponds to a row.</p>
      </td>
      <td colspan="1"> <p>LIST:</p>
        <p>- [dt=20170202/dh=00]<br/>- [dt=20170202/dh=01]<br/>- [dt=20170202/dh=02]</p>
        <p> </p>
        <p>MAP</p>
        <p>- <br/> id: 1<br/> name: name1<br/> -<br/> id: 2<br/> name: name2<br/> -<br/> id: 3<br/> name: name3<br/> -<br/> id: 4<br/> name: name4<br/> -<br/> id: 5<br/> name: name5</p>
        <p> </p>
        <p>CSV</p>
        <p>src/test/resources/expected_equals.csv</p>
      </td>
    </tr>
    <tr>
      <td colspan="1"> assert_ordered_equals</td>
      <td colspan="1">Most restrictive of all tests. This performs the exact match of the expected and actual results. Ordering is also checked in this test.</td>
      <td colspan="1"> LIST. Expects a list of list. Which means the user can provide the list of rows with comma separated column values.<p>or</p>
        <p>MAP. Expects a list of map. Which means that one has to provide each row in form of map where the keys are the column names and values are the column values. Since there can be multiple rows, hence user will provide a list of such maps where each map correspond to a row</p>
        <p>or</p>
        <p>CSV file name. Expects a csv file with header line containing the column names and the body contains the row values. Each line of the file corresponds to a row.</p>
      </td>
      <td colspan="1"> <p>LIST:</p>
        <p>- [dt=20170202/dh=00]<br/>- [dt=20170202/dh=01]<br/>- [dt=20170202/dh=02]</p>
        <p> </p>
        <p>MAP</p>
        <p>- <br/> id: 1<br/> name: name1<br/> -<br/> id: 2<br/> name: name2<br/> -<br/> id: 3<br/> name: name3<br/> -<br/> id: 4<br/> name: name4<br/> -<br/> id: 5<br/> name: name5</p>
        <p> </p>
        <p>CSV</p>
        <p>src/test/resources/expected_equals.csv</p>
      </td>
    </tr>
    <tr>
      <td colspan="1">assert_fails</td>
      <td colspan="1">This checks for the exceptions. Some time we expect an exception to be thrown while executing the query. Unfortunately in hive, the exceptions are mostly wrapped and many times, the only way to pinpoint an issue is to look at the exception message. Hence this test expects a string message and checks for the existence of that message in the actual error stack trace.</td>
      <td colspan="1">STRING message</td>
      <td colspan="1">
        <p>"org.apache.spark.sql.AnalysisException"</p>
        <p>"org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:No privilege 'Select' found for inputs { database:usage})"</p>
      </td>
    </tr>
  </tbody>
</table>

## Return types
<table>
  <tbody>
    <tr>
      <th>Name</th>
      <th>Description</th>
      <th>corresponding java type</th>
      <th>Example</th>
    </tr>
    <tr>
      <td>LIST</td>
      <td>this means that the expected value provided by the user is in form of a list of list</td>
      <td>ArrayList&lt;ArrayList&lt;Object&gt;&gt;</td>
      <td>
        <span>- [dt=20170202/dh=00]</span>
        <br/>
        <span>- [dt=20170202/dh=01]</span>
        <br/>
        <span>- [dt=20170202/dh=02]</span>
      </td>
    </tr>
    <tr>
      <td>MAP</td>
      <td>This means that the expected value provided by the user is in form of a list of map</td>
      <td>ArrayList&lt;Map&lt;String,String&gt;&gt;</td>
      <td>
        <span>- </span>
        <br/>
        <span> id: 1</span>
        <br/>
        <span> name: name1</span>
        <br/>
        <span> -</span>
        <br/>
        <span> id: 2</span>
        <br/>
        <span> name: name2</span>
        <br/>
        <span> -</span>
        <br/>
        <span> id: 3</span>
        <br/>
        <span> name: name3</span>
        <br/>
        <span> -</span>
        <br/>
        <span> id: 4</span>
        <br/>
        <span> name: name4</span>
        <br/>
        <span> -</span>
        <br/>
        <span> id: 5</span>
        <br/>
        <span> name: name5</span>
      </td>
    </tr>
    <tr>
      <td>CSV</td>
      <td>The expected value provided by user is the name of a local csv file. Should be enclosed in double quotes if there is a space in the file name so as to avoid default <span>string splitting based on space performed by java.</span>
      </td>
      <td>String</td>
      <td>
        <span>src/test/resources/expected_equals.csv</span>
      </td>
    </tr>
    <tr>
      <td>Exception</td>
      <td>The expected value provided by user is some exception message. Should be enclosed in double quotes so as to avoid default string splitting based on space performed by java.</td>
      <td>String</td>
      <td>
        <p>"org.apache.spark.sql.AnalysisException"</p>
        <p>"org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:No privilege 'Select' found for inputs { database:usage})"</p>
      </td>
    </tr>
  </tbody>
</table>

## YAML Specifications
<table>
  <tbody>
    <tr>
      <th>Name</th>
      <th>Description</th>
      <th>Required</th>
      <th>Example</th>
    </tr>
    <tr>
      <td>name</td>
      <td>Unique name of the test case</td>
      <td>true</td>
      <td>test assert permission exception</td>
    </tr>
    <tr>
      <td>setup</td>
      <td>A list of setup queries to be run before running the actual test case.</td>
      <td>false</td>
      <td>
        <p>- use temp<br/>- drop table if exists test_list</p>
      </td>
    </tr>
    <tr>
      <td>test</td>
      <td>A single query that needs to be run and whose result will be tested against expected data provided by user.</td>
      <td>true</td>
      <td>select id, name from table test_list</td>
    </tr>
    <tr>
      <td colspan="1">assert_type</td>
      <td colspan="1">One of the assert types mentioned above</td>
      <td colspan="1">true</td>
      <td colspan="1"> </td>
    </tr>
    <tr>
      <td colspan="1">teardown</td>
      <td colspan="1">A list of teardown queries to be run after the test has been performed</td>
      <td colspan="1">false</td>
      <td colspan="1">- drop table test_list</td>
    </tr>
    <tr>
      <td colspan="1">ignore_failures</td>
      <td colspan="1">boolean parameter indicating whether the tests should be marked as success or failure in case there is an error while executing setup or teardown queries. In case this has been set to true and there is an error while executing setup or teardown queries, the test will be failed.</td>
      <td colspan="1">false</td>
      <td colspan="1">false/true</td>
    </tr>
    <tr>
      <td colspan="1">return_type</td>
      <td colspan="1">One of the above mentioned return types</td>
      <td colspan="1">true</td>
      <td colspan="1"> </td>
    </tr>
    <tr>
      <td colspan="1">assert_expectations</td>
      <td colspan="1">The expected value that will be tested against the actual results obtained by running the test query. Should be in accordance with the return types. In case return type is a LIST, a list of list must be provided. In case return type is MAP, a list of maps should be provided. In case of return type being CSV, a local csv file name should be provided. In case of EXCEPTION, the expected exception message is required.</td>
      <td colspan="1">true</td>
      <td colspan="1">Mentioned above in return types and assert types.</td>
    </tr>
  </tbody>
</table>

## Executing tests
The framework packages all the necessary classes and their dependencies into an uber jar. The packaging is done using SBT. In order to run the test cases, user must provide the tests in form of YAML file while adhering to the specs mentioned above. Following parameters are supported while running
<table>
  <tbody>
    <tr>
      <th>Name</th>
      <th>Description</th>
      <th>Required</th>
      <th>Example</th>
    </tr>
    <tr>
      <td colspan="1">MAIN_JAR_LOCATION</td>
      <td colspan="1">main jar location containing the application jar</td>
      <td colspan="1">true</td>
      <td colspan="1">lib/validation-framework-assembly-1.0.jar</td>
    </tr>
    <tr>
      <td colspan="1">EXTRA_CLASSPATH</td>
      <td colspan="1">location of extra jars containing the external driver class</td>
      <td colspan="1">true in case using any external driver, false otherwise</td>
      <td colspan="1">lib/hive-jdbc-1.2.0.jar</td>
    </tr>
    <tr>
      <td>YML_FILE</td>
      <td>the test yaml file containing all the tests that are required to be run</td>
      <td>true</td>
      <td>tests.yml</td>
    </tr>
    <tr>
      <td>DB_URL</td>
      <td>the jdbc endpoint to which the framework should connect to in order to execute the tests. Note that this must be enclosed in double quotes since it may contain lots of special characters that may trigger java's parsing</td>
      <td>true</td>
      <td>"jdbc:<a>hive2://<host>:<port>/</a>;"</td>
    </tr>
    <tr>
      <td>USER_NAME</td>
      <td>the user name to be used while connecting to the jdbc end point</td>
      <td>true</td>
      <td></td>
    </tr>
    <tr>
      <td>USER_PASSWORD</td>
      <td>the password to be used while connecting to the database</td>
      <td>true</td>
      <td></td>
    </tr>
    <tr>
      <td colspan="1">LOGGER</td>
      <td colspan="1">The log4j.properties file to be used by the framework. By default the framework includes a console logger that will print all INFO and above level of logs to console. In case user wants to provide a separate logger (may be to dump the contents in a log file or to change the logger level), this option can be utilized</td>
      <td colspan="1">false</td>
      <td colspan="1">log4j.properties</td>
    </tr>
    <tr>
      <td colspan="1">ACTIVE_CONN_COUNT</td>
      <td colspan="1">The number of active connections to maintain in the connection pool. Defaults to 5</td>
      <td colspan="1">false</td>
      <td colspan="1">Some number</td>
    </tr>
    <tr>
      <td colspan="1">CONN_IDLE_TIMEOUT</td>
      <td colspan="1">Time that the framework waits for before removing the abandoned connection. Defaults to 1</td>
      <td colspan="1">false</td>
      <td colspan="1">Some number</td>
    </tr>
    <tr>
      <td colspan="1">JDBC_DRIVER</td>
      <td colspan="1">External third party JDBC driver class name</td>
      <td colspan="1">true in case user wants to use some other third party jdbc driver</td>
      <td colspan="1">driver.class.name</td>
    </tr>
  </tbody>
</table>

A utility in form of shell script is also provided to run the framework and can be downloaded from github (<a href="https://git.autodesk.com/cloudplatform-bigdata/DataUnitTest/blob/master/run.sh">https://git.autodesk.com/cloudplatform-bigdata/DataUnitTest/blob/master/run.sh</a>)

### Following command can be used to run this
./run.sh [--MAIN_JAR_LOCATION | -M &lt;main jar location containing the application jar&gt;] [--EXTRA_CLASSPATH | -E &lt;location of extra jars containing the external driver class&gt;] [--YML_FILE | -Y &lt;test yml file&gt;] [--DB_URL | -D &lt;jdbc end point&gt;] [--USER_NAME | -U &lt;user name&gt;] [--USER_PASSWORD | -P &lt;password&gt;] [--JDBC_DRIVER | -J &lt;JDBC driver class name&gt;] [--LOGGER | -L &lt;log4j properties file&gt;] [--ACTIVE_CONN_COUNT | -A &lt;active connections count&gt; ] [--CONN_IDLE_TIMEOUT | -T &lt;connection abandon timeout&gt;]

## Sample Tests
Sample test yaml files can be found in the git repo. <a href="https://git.autodesk.com/cloudplatform-bigdata/DataUnitTest/blob/master/run.sh">https://git.autodesk.com/cloudplatform-bigdata/DataUnitTest</a>. under src/test/resources

## Test results
The test results are generated in directory test-output. The tests.html file under directory validation-tests gives an html format output while the testng-results.xml gives the xml output.

## Contributing
The data-unit-test project is meant to evolve with feedback - the project and its users greatly appreciate any thoughts on ways to improve the design or features. Read below to see how you can take part and contribute:

### Contributing Guide

Read our [guide](CONTRIBUTING.md) to learn about the development process and how to work with the core team.

### License

data-unit-test is [Apache-2.0 licensed](./LICENSE)

## Future Enhancements
The current scope of the test framework is rather limited. Future enhancements can include following
- Templatization of common tests: Common tests like count, max, min, can be templatized. These templates can have placeholders for table name, column name, etc. and can be referred by the actual test yaml file. The framework should be made capable of replacing these placeholders with the values provided in the test yaml file and execute the corresponding queries. This feature will enable users to just provide the common parameters for the templates and reduce the scope of errors.
- DB Lookup: Currently, the users have to provide a static expectation data either in form of List/Map or csv file. Feature can be built where user can provide a JDBC connection and corresponding query to be executed against that jdbc endpoint. The framework should be made capable of querying the jdbc end point to fetch the expected result and convert it into appropriate format for matching with the actual results.

